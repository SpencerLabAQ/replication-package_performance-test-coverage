05/02/2025 04:02:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../saved_models/graph-codebert-finetuned/checkpoint-best-acc and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/02/2025 04:03:00 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../data/train.jsonl', output_dir='../saved_models/graph-codebert-finetuned', eval_data_file='../data/val.jsonl', test_data_file='../data/test.jsonl', model_type='roberta', model_name_or_path='../saved_models/graph-codebert-finetuned/checkpoint-best-acc', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/graphcodebert-base', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
/NFSHOME/mimran/graph-codebert/code/run.py:576: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
05/02/2025 04:03:13 - INFO - __main__ -   ***** Running evaluation *****
05/02/2025 04:03:13 - INFO - __main__ -     Num examples = 17770
05/02/2025 04:03:13 - INFO - __main__ -     Batch size = 64
05/02/2025 04:12:52 - INFO - __main__ -   ***** Eval results *****
05/02/2025 04:12:52 - INFO - __main__ -     eval_acc = 0.9458
05/02/2025 04:12:52 - INFO - __main__ -     eval_loss = 0.1747
/NFSHOME/mimran/graph-codebert/code/run.py:585: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path))
05/02/2025 04:13:01 - INFO - __main__ -   ***** Running Test *****
05/02/2025 04:13:01 - INFO - __main__ -     Num examples = 17770
05/02/2025 04:13:01 - INFO - __main__ -     Batch size = 64
  0%|          | 0/278 [00:00<?, ?it/s]  0%|          | 1/278 [00:02<09:34,  2.07s/it]  1%|          | 2/278 [00:04<09:32,  2.07s/it]  1%|          | 3/278 [00:06<09:30,  2.07s/it]  1%|▏         | 4/278 [00:08<09:27,  2.07s/it]  2%|▏         | 5/278 [00:10<09:25,  2.07s/it]  2%|▏         | 6/278 [00:12<09:23,  2.07s/it]  3%|▎         | 7/278 [00:14<09:21,  2.07s/it]  3%|▎         | 8/278 [00:16<09:19,  2.07s/it]  3%|▎         | 9/278 [00:18<09:17,  2.07s/it]  4%|▎         | 10/278 [00:20<09:15,  2.07s/it]  4%|▍         | 11/278 [00:22<09:13,  2.07s/it]  4%|▍         | 12/278 [00:24<09:11,  2.07s/it]  5%|▍         | 13/278 [00:26<09:09,  2.07s/it]  5%|▌         | 14/278 [00:29<09:07,  2.07s/it]  5%|▌         | 15/278 [00:31<09:05,  2.07s/it]  6%|▌         | 16/278 [00:33<09:03,  2.07s/it]  6%|▌         | 17/278 [00:35<09:01,  2.07s/it]  6%|▋         | 18/278 [00:37<08:58,  2.07s/it]  7%|▋         | 19/278 [00:39<08:56,  2.07s/it]  7%|▋         | 20/278 [00:41<08:54,  2.07s/it]  8%|▊         | 21/278 [00:43<08:52,  2.07s/it]  8%|▊         | 22/278 [00:45<08:50,  2.07s/it]  8%|▊         | 23/278 [00:47<08:48,  2.07s/it]  9%|▊         | 24/278 [00:49<08:46,  2.07s/it]  9%|▉         | 25/278 [00:51<08:44,  2.07s/it]  9%|▉         | 26/278 [00:53<08:42,  2.07s/it] 10%|▉         | 27/278 [00:55<08:40,  2.07s/it] 10%|█         | 28/278 [00:58<08:38,  2.07s/it] 10%|█         | 29/278 [01:00<08:36,  2.07s/it] 11%|█         | 30/278 [01:02<08:34,  2.07s/it] 11%|█         | 31/278 [01:04<08:32,  2.07s/it] 12%|█▏        | 32/278 [01:06<08:29,  2.07s/it] 12%|█▏        | 33/278 [01:08<08:27,  2.07s/it] 12%|█▏        | 34/278 [01:10<08:25,  2.07s/it] 13%|█▎        | 35/278 [01:12<08:23,  2.07s/it] 13%|█▎        | 36/278 [01:14<08:21,  2.07s/it] 13%|█▎        | 37/278 [01:16<08:19,  2.07s/it] 14%|█▎        | 38/278 [01:18<08:17,  2.07s/it] 14%|█▍        | 39/278 [01:20<08:15,  2.07s/it] 14%|█▍        | 40/278 [01:22<08:13,  2.07s/it] 15%|█▍        | 41/278 [01:24<08:11,  2.07s/it] 15%|█▌        | 42/278 [01:27<08:09,  2.07s/it] 15%|█▌        | 43/278 [01:29<08:07,  2.07s/it] 16%|█▌        | 44/278 [01:31<08:05,  2.07s/it] 16%|█▌        | 45/278 [01:33<08:03,  2.07s/it] 17%|█▋        | 46/278 [01:35<08:01,  2.07s/it] 17%|█▋        | 47/278 [01:37<07:58,  2.07s/it] 17%|█▋        | 48/278 [01:39<07:56,  2.07s/it] 18%|█▊        | 49/278 [01:41<07:54,  2.07s/it] 18%|█▊        | 50/278 [01:43<07:52,  2.07s/it] 18%|█▊        | 51/278 [01:45<07:50,  2.07s/it] 19%|█▊        | 52/278 [01:47<07:48,  2.07s/it] 19%|█▉        | 53/278 [01:49<07:46,  2.07s/it] 19%|█▉        | 54/278 [01:51<07:44,  2.07s/it] 20%|█▉        | 55/278 [01:54<07:42,  2.07s/it] 20%|██        | 56/278 [01:56<07:40,  2.07s/it] 21%|██        | 57/278 [01:58<07:38,  2.07s/it] 21%|██        | 58/278 [02:00<07:36,  2.07s/it] 21%|██        | 59/278 [02:02<07:34,  2.07s/it] 22%|██▏       | 60/278 [02:04<07:31,  2.07s/it] 22%|██▏       | 61/278 [02:06<07:29,  2.07s/it] 22%|██▏       | 62/278 [02:08<07:27,  2.07s/it] 23%|██▎       | 63/278 [02:10<07:25,  2.07s/it] 23%|██▎       | 64/278 [02:12<07:23,  2.07s/it] 23%|██▎       | 65/278 [02:14<07:21,  2.07s/it] 24%|██▎       | 66/278 [02:16<07:19,  2.07s/it] 24%|██▍       | 67/278 [02:18<07:17,  2.07s/it] 24%|██▍       | 68/278 [02:20<07:15,  2.07s/it] 25%|██▍       | 69/278 [02:23<07:13,  2.07s/it] 25%|██▌       | 70/278 [02:25<07:11,  2.07s/it] 26%|██▌       | 71/278 [02:27<07:09,  2.07s/it] 26%|██▌       | 72/278 [02:29<07:06,  2.07s/it] 26%|██▋       | 73/278 [02:31<07:04,  2.07s/it] 27%|██▋       | 74/278 [02:33<07:02,  2.07s/it] 27%|██▋       | 75/278 [02:35<07:00,  2.07s/it] 27%|██▋       | 76/278 [02:37<06:58,  2.07s/it] 28%|██▊       | 77/278 [02:39<06:56,  2.07s/it] 28%|██▊       | 78/278 [02:41<06:54,  2.07s/it] 28%|██▊       | 79/278 [02:43<06:52,  2.07s/it] 29%|██▉       | 80/278 [02:45<06:50,  2.07s/it] 29%|██▉       | 81/278 [02:47<06:48,  2.07s/it] 29%|██▉       | 82/278 [02:49<06:46,  2.07s/it] 30%|██▉       | 83/278 [02:52<06:44,  2.07s/it] 30%|███       | 84/278 [02:54<06:42,  2.07s/it] 31%|███       | 85/278 [02:56<06:39,  2.07s/it] 31%|███       | 86/278 [02:58<06:37,  2.07s/it] 31%|███▏      | 87/278 [03:00<06:35,  2.07s/it] 32%|███▏      | 88/278 [03:02<06:33,  2.07s/it] 32%|███▏      | 89/278 [03:04<06:31,  2.07s/it] 32%|███▏      | 90/278 [03:06<06:29,  2.07s/it] 33%|███▎      | 91/278 [03:08<06:27,  2.07s/it] 33%|███▎      | 92/278 [03:10<06:25,  2.07s/it] 33%|███▎      | 93/278 [03:12<06:23,  2.07s/it] 34%|███▍      | 94/278 [03:14<06:21,  2.07s/it] 34%|███▍      | 95/278 [03:16<06:19,  2.07s/it] 35%|███▍      | 96/278 [03:19<06:17,  2.07s/it] 35%|███▍      | 97/278 [03:21<06:15,  2.07s/it] 35%|███▌      | 98/278 [03:23<06:13,  2.07s/it] 36%|███▌      | 99/278 [03:25<06:11,  2.07s/it] 36%|███▌      | 100/278 [03:27<06:09,  2.07s/it] 36%|███▋      | 101/278 [03:29<06:06,  2.07s/it] 37%|███▋      | 102/278 [03:31<06:04,  2.07s/it] 37%|███▋      | 103/278 [03:33<06:02,  2.07s/it] 37%|███▋      | 104/278 [03:35<06:00,  2.07s/it] 38%|███▊      | 105/278 [03:37<05:58,  2.07s/it] 38%|███▊      | 106/278 [03:39<05:56,  2.07s/it] 38%|███▊      | 107/278 [03:41<05:54,  2.07s/it] 39%|███▉      | 108/278 [03:43<05:52,  2.07s/it] 39%|███▉      | 109/278 [03:45<05:50,  2.07s/it] 40%|███▉      | 110/278 [03:48<05:48,  2.07s/it] 40%|███▉      | 111/278 [03:50<05:46,  2.07s/it] 40%|████      | 112/278 [03:52<05:44,  2.07s/it] 41%|████      | 113/278 [03:54<05:42,  2.07s/it] 41%|████      | 114/278 [03:56<05:40,  2.07s/it] 41%|████▏     | 115/278 [03:58<05:37,  2.07s/it] 42%|████▏     | 116/278 [04:00<05:35,  2.07s/it] 42%|████▏     | 117/278 [04:02<05:33,  2.07s/it] 42%|████▏     | 118/278 [04:04<05:31,  2.07s/it] 43%|████▎     | 119/278 [04:06<05:29,  2.07s/it] 43%|████▎     | 120/278 [04:08<05:27,  2.07s/it] 44%|████▎     | 121/278 [04:10<05:25,  2.07s/it] 44%|████▍     | 122/278 [04:12<05:23,  2.07s/it] 44%|████▍     | 123/278 [04:14<05:21,  2.07s/it] 45%|████▍     | 124/278 [04:17<05:19,  2.07s/it] 45%|████▍     | 125/278 [04:19<05:17,  2.07s/it] 45%|████▌     | 126/278 [04:21<05:15,  2.07s/it] 46%|████▌     | 127/278 [04:23<05:13,  2.07s/it] 46%|████▌     | 128/278 [04:25<05:10,  2.07s/it] 46%|████▋     | 129/278 [04:27<05:08,  2.07s/it] 47%|████▋     | 130/278 [04:29<05:06,  2.07s/it] 47%|████▋     | 131/278 [04:31<05:04,  2.07s/it] 47%|████▋     | 132/278 [04:33<05:02,  2.07s/it] 48%|████▊     | 133/278 [04:35<05:00,  2.07s/it] 48%|████▊     | 134/278 [04:37<04:58,  2.07s/it] 49%|████▊     | 135/278 [04:39<04:56,  2.07s/it] 49%|████▉     | 136/278 [04:41<04:54,  2.07s/it] 49%|████▉     | 137/278 [04:44<04:52,  2.07s/it] 50%|████▉     | 138/278 [04:46<04:50,  2.07s/it] 50%|█████     | 139/278 [04:48<04:48,  2.07s/it] 50%|█████     | 140/278 [04:50<04:46,  2.07s/it] 51%|█████     | 141/278 [04:52<04:44,  2.07s/it] 51%|█████     | 142/278 [04:54<04:41,  2.07s/it] 51%|█████▏    | 143/278 [04:56<04:39,  2.07s/it] 52%|█████▏    | 144/278 [04:58<04:37,  2.07s/it] 52%|█████▏    | 145/278 [05:00<04:35,  2.07s/it] 53%|█████▎    | 146/278 [05:02<04:33,  2.07s/it] 53%|█████▎    | 147/278 [05:04<04:31,  2.07s/it] 53%|█████▎    | 148/278 [05:06<04:29,  2.07s/it] 54%|█████▎    | 149/278 [05:08<04:27,  2.07s/it] 54%|█████▍    | 150/278 [05:10<04:25,  2.07s/it] 54%|█████▍    | 151/278 [05:13<04:23,  2.07s/it] 55%|█████▍    | 152/278 [05:15<04:21,  2.07s/it] 55%|█████▌    | 153/278 [05:17<04:19,  2.07s/it] 55%|█████▌    | 154/278 [05:19<04:17,  2.07s/it] 56%|█████▌    | 155/278 [05:21<04:15,  2.07s/it] 56%|█████▌    | 156/278 [05:23<04:12,  2.07s/it] 56%|█████▋    | 157/278 [05:25<04:10,  2.07s/it] 57%|█████▋    | 158/278 [05:27<04:08,  2.07s/it] 57%|█████▋    | 159/278 [05:29<04:06,  2.07s/it] 58%|█████▊    | 160/278 [05:31<04:04,  2.07s/it] 58%|█████▊    | 161/278 [05:33<04:02,  2.07s/it] 58%|█████▊    | 162/278 [05:35<04:00,  2.07s/it] 59%|█████▊    | 163/278 [05:37<03:58,  2.07s/it] 59%|█████▉    | 164/278 [05:39<03:56,  2.07s/it] 59%|█████▉    | 165/278 [05:42<03:54,  2.07s/it] 60%|█████▉    | 166/278 [05:44<03:52,  2.07s/it] 60%|██████    | 167/278 [05:46<03:50,  2.07s/it] 60%|██████    | 168/278 [05:48<03:48,  2.07s/it] 61%|██████    | 169/278 [05:50<03:46,  2.07s/it] 61%|██████    | 170/278 [05:52<03:43,  2.07s/it] 62%|██████▏   | 171/278 [05:54<03:41,  2.07s/it] 62%|██████▏   | 172/278 [05:56<03:39,  2.07s/it] 62%|██████▏   | 173/278 [05:58<03:37,  2.07s/it] 63%|██████▎   | 174/278 [06:00<03:35,  2.07s/it] 63%|██████▎   | 175/278 [06:02<03:33,  2.07s/it] 63%|██████▎   | 176/278 [06:04<03:31,  2.07s/it] 64%|██████▎   | 177/278 [06:06<03:29,  2.07s/it] 64%|██████▍   | 178/278 [06:09<03:27,  2.07s/it] 64%|██████▍   | 179/278 [06:11<03:25,  2.07s/it] 65%|██████▍   | 180/278 [06:13<03:23,  2.07s/it] 65%|██████▌   | 181/278 [06:15<03:21,  2.07s/it] 65%|██████▌   | 182/278 [06:17<03:18,  2.07s/it] 66%|██████▌   | 183/278 [06:19<03:16,  2.07s/it] 66%|██████▌   | 184/278 [06:21<03:14,  2.07s/it] 67%|██████▋   | 185/278 [06:23<03:12,  2.07s/it] 67%|██████▋   | 186/278 [06:25<03:10,  2.07s/it] 67%|██████▋   | 187/278 [06:27<03:08,  2.07s/it] 68%|██████▊   | 188/278 [06:29<03:06,  2.07s/it] 68%|██████▊   | 189/278 [06:31<03:04,  2.07s/it] 68%|██████▊   | 190/278 [06:33<03:02,  2.07s/it] 69%|██████▊   | 191/278 [06:35<03:00,  2.07s/it] 69%|██████▉   | 192/278 [06:38<02:58,  2.07s/it] 69%|██████▉   | 193/278 [06:40<02:56,  2.07s/it] 70%|██████▉   | 194/278 [06:42<02:54,  2.07s/it] 70%|███████   | 195/278 [06:44<02:52,  2.07s/it] 71%|███████   | 196/278 [06:46<02:49,  2.07s/it] 71%|███████   | 197/278 [06:48<02:47,  2.07s/it] 71%|███████   | 198/278 [06:50<02:45,  2.07s/it] 72%|███████▏  | 199/278 [06:52<02:43,  2.07s/it] 72%|███████▏  | 200/278 [06:54<02:41,  2.07s/it] 72%|███████▏  | 201/278 [06:56<02:39,  2.07s/it] 73%|███████▎  | 202/278 [06:58<02:37,  2.07s/it] 73%|███████▎  | 203/278 [07:00<02:35,  2.07s/it] 73%|███████▎  | 204/278 [07:02<02:33,  2.07s/it] 74%|███████▎  | 205/278 [07:04<02:31,  2.07s/it] 74%|███████▍  | 206/278 [07:07<02:29,  2.07s/it] 74%|███████▍  | 207/278 [07:09<02:27,  2.07s/it] 75%|███████▍  | 208/278 [07:11<02:25,  2.07s/it] 75%|███████▌  | 209/278 [07:13<02:23,  2.07s/it] 76%|███████▌  | 210/278 [07:15<02:20,  2.07s/it] 76%|███████▌  | 211/278 [07:17<02:18,  2.07s/it] 76%|███████▋  | 212/278 [07:19<02:16,  2.07s/it] 77%|███████▋  | 213/278 [07:21<02:14,  2.07s/it] 77%|███████▋  | 214/278 [07:23<02:12,  2.07s/it] 77%|███████▋  | 215/278 [07:25<02:10,  2.07s/it] 78%|███████▊  | 216/278 [07:27<02:08,  2.07s/it] 78%|███████▊  | 217/278 [07:29<02:06,  2.07s/it] 78%|███████▊  | 218/278 [07:31<02:04,  2.07s/it] 79%|███████▉  | 219/278 [07:34<02:02,  2.07s/it] 79%|███████▉  | 220/278 [07:36<02:00,  2.07s/it] 79%|███████▉  | 221/278 [07:38<01:58,  2.07s/it] 80%|███████▉  | 222/278 [07:40<01:56,  2.07s/it] 80%|████████  | 223/278 [07:42<01:53,  2.07s/it] 81%|████████  | 224/278 [07:44<01:51,  2.07s/it] 81%|████████  | 225/278 [07:46<01:49,  2.07s/it] 81%|████████▏ | 226/278 [07:48<01:47,  2.07s/it] 82%|████████▏ | 227/278 [07:50<01:45,  2.07s/it] 82%|████████▏ | 228/278 [07:52<01:43,  2.07s/it] 82%|████████▏ | 229/278 [07:54<01:41,  2.07s/it] 83%|████████▎ | 230/278 [07:56<01:39,  2.07s/it] 83%|████████▎ | 231/278 [07:58<01:37,  2.07s/it] 83%|████████▎ | 232/278 [08:00<01:35,  2.07s/it] 84%|████████▍ | 233/278 [08:03<01:33,  2.07s/it] 84%|████████▍ | 234/278 [08:05<01:31,  2.07s/it] 85%|████████▍ | 235/278 [08:07<01:29,  2.07s/it] 85%|████████▍ | 236/278 [08:09<01:27,  2.07s/it] 85%|████████▌ | 237/278 [08:11<01:24,  2.07s/it] 86%|████████▌ | 238/278 [08:13<01:22,  2.07s/it] 86%|████████▌ | 239/278 [08:15<01:20,  2.07s/it] 86%|████████▋ | 240/278 [08:17<01:18,  2.07s/it] 87%|████████▋ | 241/278 [08:19<01:16,  2.07s/it] 87%|████████▋ | 242/278 [08:21<01:14,  2.07s/it] 87%|████████▋ | 243/278 [08:23<01:12,  2.07s/it] 88%|████████▊ | 244/278 [08:25<01:10,  2.07s/it] 88%|████████▊ | 245/278 [08:27<01:08,  2.07s/it] 88%|████████▊ | 246/278 [08:29<01:06,  2.07s/it] 89%|████████▉ | 247/278 [08:32<01:04,  2.07s/it] 89%|████████▉ | 248/278 [08:34<01:02,  2.07s/it] 90%|████████▉ | 249/278 [08:36<01:00,  2.07s/it] 90%|████████▉ | 250/278 [08:38<00:58,  2.07s/it] 90%|█████████ | 251/278 [08:40<00:55,  2.07s/it] 91%|█████████ | 252/278 [08:42<00:53,  2.07s/it] 91%|█████████ | 253/278 [08:44<00:51,  2.07s/it] 91%|█████████▏| 254/278 [08:46<00:49,  2.07s/it] 92%|█████████▏| 255/278 [08:48<00:47,  2.07s/it] 92%|█████████▏| 256/278 [08:50<00:45,  2.07s/it] 92%|█████████▏| 257/278 [08:52<00:43,  2.07s/it] 93%|█████████▎| 258/278 [08:54<00:41,  2.07s/it] 93%|█████████▎| 259/278 [08:56<00:39,  2.07s/it] 94%|█████████▎| 260/278 [08:58<00:37,  2.07s/it] 94%|█████████▍| 261/278 [09:01<00:35,  2.07s/it] 94%|█████████▍| 262/278 [09:03<00:33,  2.07s/it] 95%|█████████▍| 263/278 [09:05<00:31,  2.07s/it] 95%|█████████▍| 264/278 [09:07<00:29,  2.07s/it] 95%|█████████▌| 265/278 [09:09<00:26,  2.07s/it] 96%|█████████▌| 266/278 [09:11<00:24,  2.07s/it] 96%|█████████▌| 267/278 [09:13<00:22,  2.07s/it] 96%|█████████▋| 268/278 [09:15<00:20,  2.07s/it] 97%|█████████▋| 269/278 [09:17<00:18,  2.07s/it] 97%|█████████▋| 270/278 [09:19<00:16,  2.07s/it] 97%|█████████▋| 271/278 [09:21<00:14,  2.07s/it] 98%|█████████▊| 272/278 [09:23<00:12,  2.07s/it] 98%|█████████▊| 273/278 [09:25<00:10,  2.07s/it] 99%|█████████▊| 274/278 [09:27<00:08,  2.07s/it] 99%|█████████▉| 275/278 [09:30<00:06,  2.07s/it] 99%|█████████▉| 276/278 [09:32<00:04,  2.07s/it]100%|█████████▉| 277/278 [09:34<00:02,  2.07s/it]100%|██████████| 278/278 [09:35<00:00,  1.85s/it]100%|██████████| 278/278 [09:35<00:00,  2.07s/it]
